from fastapi import APIRouter, HTTPException
from qdrant_client import QdrantClient
from datetime import datetime
import pytz
from config import settings
import structlog

logger = structlog.get_logger()
router = APIRouter()

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

def get_kst_now():
    """í•œêµ­ ì‹œê°„ìœ¼ë¡œ í˜„ìž¬ ì‹œê°„ ë°˜í™˜"""
    return datetime.now(KST)


@router.get("/status")
async def get_db_status():
    """Get database status and recent crawling info"""
    try:
        # Qdrant ì»¬ë ‰ì…˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        qdrant_client = QdrantClient(
            url=settings.qdrant_host,
            api_key=settings.qdrant_api_key
        )
        
        logger.info(f"Connecting to Qdrant at {settings.qdrant_host}:{settings.qdrant_port}")
        
        # ì»¬ë ‰ì…˜ ì •ë³´ - ì •í™•í•œ ê°œìˆ˜ ê°€ì ¸ì˜¤ê¸°
        try:
            collection_info = qdrant_client.get_collection(settings.qdrant_collection_name)
            total_points = collection_info.points_count
            logger.info(f"âœ… Collection '{settings.qdrant_collection_name}' found with {total_points} points")
        except Exception as e:
            logger.error(f"âŒ Failed to get collection info: {e}")
            # ëŒ€ì•ˆ: ì „ì²´ ì  ê°œìˆ˜ë¥¼ ì •í™•ížˆ ì„¸ê¸° (scroll ë°˜ë³µ)
            try:
                total_points = 0
                next_offset = None
                
                while True:
                    scroll_result = qdrant_client.scroll(
                        collection_name=settings.qdrant_collection_name,
                        limit=1000,
                        offset=next_offset,
                        with_payload=False,
                        with_vectors=False
                    )
                    
                    points, next_offset = scroll_result
                    total_points += len(points)
                    
                    if next_offset is None:  # ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŒ
                        break
                        
                logger.info(f"âœ… Fallback: Counted {total_points} total points via scroll")
            except Exception as scroll_e:
                logger.error(f"âŒ Scroll counting failed: {scroll_e}")
                total_points = 0
        
        # ìµœê·¼ ì €ìž¥ëœ ë°ì´í„° ì¡°íšŒ (ë” ë§Žì´ ê°€ì ¸ì™€ì„œ ì •ë ¬)
        try:
            logger.info("ðŸ“‹ Fetching recent data...")
            # ì—¬ëŸ¬ ë²ˆ scrollí•´ì„œ ë” ë§Žì€ ë°ì´í„° ìˆ˜ì§‘
            all_points = []
            next_offset = None
            
            # ìµœëŒ€ 1000ê°œê¹Œì§€ ê°€ì ¸ì˜¤ê¸° (10ë²ˆ * 100ê°œ)
            for _ in range(10):
                scroll_result = qdrant_client.scroll(
                    collection_name=settings.qdrant_collection_name,
                    limit=100,
                    offset=next_offset,
                    with_payload=True,
                    with_vectors=False
                )
                points, next_offset = scroll_result
                all_points.extend(points)
                
                if next_offset is None or len(all_points) >= 1000:
                    break
            
            recent_data = all_points
            
            logger.info(f"ðŸ“‹ Found {len(recent_data)} recent data points")
            
            # updated_at ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
            recent_items = []
            for point in recent_data:
                if point.payload:
                    item = {
                        "url": point.payload.get("url", "Unknown"),
                        "updated_at": point.payload.get("updated_at", "Unknown"),
                        "chunk_index": point.payload.get("chunk_index", 0),
                        "total_chunks": point.payload.get("total_chunks", 1)
                    }
                    recent_items.append(item)
                    logger.debug(f"ðŸ“„ Item: {item['url']} - {item['updated_at']}")
            
            # ì‹œê°„ìˆœ ì •ë ¬ (ìµœì‹ ì´ ë¨¼ì €, Unknownì€ ë‚˜ì¤‘ì—)
            def sort_key(x):
                if x["updated_at"] == "Unknown":
                    return (1, datetime.min)  # Unknownì€ ë§¨ ë’¤ë¡œ
                try:
                    # ì‹œê°„ ë¬¸ìžì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±í•´ì„œ ì •ë ¬
                    dt_str = x["updated_at"]
                    # ì‹œê°„ëŒ€ ì •ë³´ ì²˜ë¦¬
                    if '+' in dt_str or 'Z' in dt_str:
                        dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
                    else:
                        # naive datetimeì€ KSTë¡œ ê°€ì •
                        dt = datetime.fromisoformat(dt_str)
                        if dt.tzinfo is None:
                            dt = KST.localize(dt)
                    return (0, dt.timestamp())  # timestampë¡œ ë¹„êµ
                except Exception as e:
                    logger.debug(f"Failed to parse date {x['updated_at']}: {e}")
                    return (1, datetime.min.timestamp())  # íŒŒì‹± ì‹¤íŒ¨ì‹œ ë’¤ë¡œ
                    
            recent_items.sort(key=sort_key, reverse=True)  # ìµœì‹ ì´ ë¨¼ì €
            
            # ìƒìœ„ 10ê°œë§Œ ì„ íƒ
            recent_items = recent_items[:10]
            logger.info(f"ðŸ“‹ Processed {len(recent_items)} items (showing top 10)")
            
        except Exception as e:
            recent_items = []
            logger.error(f"âŒ Failed to get recent data: {e}")
        
        return {
            "status": "healthy",
            "total_documents": total_points,
            "collection_name": settings.qdrant_collection_name,
            "recent_updates": recent_items[:5],  # ìµœê·¼ 5ê°œë§Œ
            "last_checked": get_kst_now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Failed to get DB status: {e}")
        return {
            "status": "error",
            "error": str(e),
            "last_checked": get_kst_now().isoformat()
        }


@router.get("/search-url")
async def search_url(url: str):
    """Search if a URL exists in the database using efficient filtering"""
    try:
        if not url:
            return {
                "error": "URL is required",
                "found": False
            }
        
        # URL ì •ê·œí™” (trailing slash ì œê±° ë“±)
        normalized_url = url.rstrip('/')
        
        # Qdrant í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        qdrant_client = QdrantClient(
            url=settings.qdrant_host,
            api_key=settings.qdrant_api_key
        )
        
        logger.info(f"ðŸ” Searching for URL: {normalized_url}")
        
        # íš¨ìœ¨ì ì¸ í•„í„°ë§ì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
        found_urls = []
        total_checked = 0
        
        try:
            # URLë¡œ ì‹œìž‘í•˜ëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰
            scroll_result = qdrant_client.scroll(
                collection_name=settings.qdrant_collection_name,
                scroll_filter={
                    "must": [
                        {
                            "key": "url",
                            "match": {
                                "text": normalized_url
                            }
                        }
                    ]
                },
                limit=100,  # ì¶©ë¶„í•œ ìˆ˜ì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜´
                with_payload=True,
                with_vectors=False
            )
            
            points, _ = scroll_result
            total_checked = len(points)
            
            # ê²°ê³¼ ì²˜ë¦¬
            for point in points:
                if point.payload and 'url' in point.payload:
                    doc_url = point.payload['url']
                    # URLì´ ìž…ë ¥ëœ URLë¡œ ì‹œìž‘í•˜ëŠ”ì§€ í™•ì¸ (ì¶”ê°€ ê²€ì¦)
                    if doc_url.startswith(normalized_url):
                        found_item = {
                            "url": doc_url,
                            "updated_at": point.payload.get('updated_at', 'Unknown'),
                            "chunk_index": point.payload.get('chunk_index', 0),
                            "total_chunks": point.payload.get('total_chunks', 1)
                        }
                        
                        # ì¤‘ë³µ ì œê±° (ê°™ì€ URLì˜ ë‹¤ë¥¸ ì²­í¬ëŠ” í•œ ë²ˆë§Œ í‘œì‹œ)
                        if not any(item['url'] == doc_url for item in found_urls):
                            found_urls.append(found_item)
                            logger.info(f"âœ… Found matching URL: {doc_url}")
            
            logger.info(f"ðŸ” Efficient search complete. Found {len(found_urls)} matching URLs out of {total_checked} checked")
            
        except Exception as filter_error:
            logger.warning(f"Filter search failed, falling back to full scan: {filter_error}")
            # í•„í„° ê²€ìƒ‰ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
            found_urls = []
            total_checked = 0
            next_offset = None
            
            while True:
                scroll_result = qdrant_client.scroll(
                    collection_name=settings.qdrant_collection_name,
                    limit=100,
                    offset=next_offset,
                    with_payload=True,
                    with_vectors=False
                )
                
                points, next_offset = scroll_result
                
                for point in points:
                    total_checked += 1
                    if point.payload and 'url' in point.payload:
                        doc_url = point.payload['url']
                        if doc_url.startswith(normalized_url):
                            found_item = {
                                "url": doc_url,
                                "updated_at": point.payload.get('updated_at', 'Unknown'),
                                "chunk_index": point.payload.get('chunk_index', 0),
                                "total_chunks": point.payload.get('total_chunks', 1)
                            }
                            
                            if not any(item['url'] == doc_url for item in found_urls):
                                found_urls.append(found_item)
                                logger.info(f"âœ… Found matching URL: {doc_url}")
                
                if next_offset is None or total_checked >= 1000:
                    break
        
        # ì‹œê°„ìˆœ ì •ë ¬ (ìµœì‹  ë¨¼ì €)
        def sort_key(x):
            if x["updated_at"] == "Unknown":
                return datetime.min.timestamp()
            try:
                dt_str = x["updated_at"]
                if '+' in dt_str or 'Z' in dt_str:
                    dt = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))
                else:
                    dt = datetime.fromisoformat(dt_str)
                    if dt.tzinfo is None:
                        dt = KST.localize(dt)
                return dt.timestamp()
            except:
                return datetime.min.timestamp()
        
        found_urls.sort(key=sort_key, reverse=True)
        
        # ìµœëŒ€ 20ê°œê¹Œì§€ë§Œ ë°˜í™˜
        found_urls = found_urls[:20]
        
        return {
            "search_url": normalized_url,
            "found": len(found_urls) > 0,
            "count": len(found_urls),
            "total_checked": total_checked,
            "matching_urls": found_urls,
            "checked_at": get_kst_now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Failed to search URL: {e}")
        return {
            "error": str(e),
            "found": False,
            "checked_at": get_kst_now().isoformat()
        }